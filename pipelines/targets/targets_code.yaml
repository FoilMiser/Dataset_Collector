# targets_code.yaml
# Code-specialized targets for the ethical dataset pipeline (v2 layout)
schema_version: "0.9"
updated_utc: "2025-12-30"
notes: >
  Code-focused corpus plan. GREEN sources are safe to ingest automatically.
  YELLOW sources require record-level licensing, attribution bundling, and/or extra ToU review.
  RED sources are intentionally excluded (see CODE_PIPELINE_ADAPTATION.md).

companion_files:
  license_map:
    - "../../configs/common/license_map.yaml"
  field_schemas:
    - "../../configs/common/field_schemas.yaml"
  denylist:
    - "../../configs/common/denylist.yaml"

globals:
  raw_root: "/data/code/raw"
  screened_yellow_root: "/data/code/screened_yellow"
  combined_root: "/data/code/combined"
  ledger_root: "/data/code/_ledger"
  pitches_root: "/data/code/_pitches"
  manifests_root: "/data/code/_manifests"
  queues_root: "/data/code/_queues"
  catalogs_root: "/data/code/_catalogs"
  logs_root: "/data/code/_logs"

  require_yellow_signoff: true

  sharding:
    max_records_per_shard: 50000
    compression: gzip

  screening:
    min_chars: 50
    max_chars: 20000
    text_field_candidates: ["text","code","content"]
    record_license_field_candidates: ["license","license_spdx"]
    require_record_license: false
    allow_spdx: ["MIT","Apache-2.0","CC-BY-4.0","CC0-1.0","BSD-3-Clause","ISC"]
    deny_phrases: ["noai","no tdm","no machine learning"]

  # For code, near-duplicate detection is often worthwhile (vendor folders, forks, copy/paste).
  near_duplicate_detection:
    enabled: true
    method: "minhash_lsh"
    num_perm: 128
    threshold: 0.85
    emit_duplicate_groups: true

  parquet_output:
    enabled: false
    compression: "snappy"
    row_group_size: 100000

  download_defaults:
    retry_max: 3
    retry_backoff_base: 2.0
    retry_backoff_max: 120
    timeout_connect: 30
    timeout_read: 600
    verify_checksums: true
    enable_resume: true
    max_concurrent_workers: 4
    user_agent: "code-corpus-pipeline/0.1"

  # Default text chunking (for docs, READMEs, issue threads, etc.)
  text_processing_defaults:
    max_chunk_chars: 7000
    min_chunk_chars: 400
    drop_tables: false
    drop_references_section: false
    keep_citations_inline: true
    include_section_headers: true
    include_figure_captions: false
    include_table_captions: false

  # Default code chunking (used by code_worker.py)
  code_processing_defaults:
    max_file_bytes: 250000       # avoid huge vendored blobs
    min_file_bytes: 64
    preferred_chunker: "ast_then_fallback"
    max_chunk_chars: 12000
    min_chunk_chars: 200
    include_comments: true
    strip_trailing_whitespace: true
    normalize_newlines: true
    languages_allowlist: ["python","javascript","typescript","java","go","rust","c","cpp","csharp","php","ruby","kotlin","swift","scala","sql","bash","powershell","html","css"]
    path_deny_globs:
      - "**/node_modules/**"
      - "**/vendor/**"
      - "**/third_party/**"
      - "**/dist/**"
      - "**/build/**"
      - "**/target/**"
      - "**/.git/**"
      - "**/*.min.*"
      - "**/*.lock"
      - "**/*.svg"
      - "**/*.png"
      - "**/*.jpg"
      - "**/*.pdf"

  statistics_defaults:
    estimate_tokens: true
    token_model: "cl100k_base"
    collect_property_distributions: true
    sample_size_for_distributions: 20000

# Resolvers: informational metadata for humans; the driver currently relies on download.strategy.
resolver_defs: !include ../../configs/common/resolvers.yaml

resolvers:
  http:
    <<: *http_resolver
    description: "Direct HTTP(S) downloads"
  git:
    <<: *git_resolver
    description: "Clone a git repo (for small-ish sources like PEPs/specs)"
    shallow_default: true
  huggingface_datasets:
    <<: *huggingface_datasets
    description: "Download via Hugging Face datasets (note: large sources may require streaming/sharding work)"

targets:
  # -----------------------------
  # GREEN (auto-ingest)
  # -----------------------------

  - id: "synthetic_code_problems_v1"
    name: "Synthetic Code Problems (internal generator output)"
    enabled: false
    bucket_hint: "GREEN"
    license_profile: "permissive"
    license_evidence:
      spdx_hint: "Proprietary"
      url: "internal://synthetic"
      notes: "You own/generated this content; keep generator logs as provenance."
    data_type: ["code","instruction","unit_tests","solutions"]
    download:
      strategy: "none"
    output:
      pool: "permissive"
      format: "jsonl"

  - id: "openai_humaneval"
    name: "OpenAI HumanEval (evaluation set)"
    enabled: true
    bucket_hint: "GREEN"
    license_profile: "permissive"
    license_evidence:
      spdx_hint: "MIT"
      url: "https://huggingface.co/datasets/openai/openai_humaneval"
    data_type: ["code","unit_tests","instructions"]
    download:
      strategy: "huggingface_datasets"
      config:
        dataset_id: "openai/openai_humaneval"
    canonicalize:
      text_field_candidates: ["prompt", "canonical_solution", "solution", "text"]

  - id: "python_peps"
    name: "Python Enhancement Proposals (PEPs)"
    enabled: true
    bucket_hint: "GREEN"
    license_profile: "permissive"
    license_evidence:
      spdx_hint: "CC0-1.0"
      url: "https://peps.python.org/pep-0001/"
      notes: "PEP 1 states PEP documents are public domain or CC0-1.0 (whichever more permissive)."
    data_type: ["documentation","spec","code_examples"]
    download:
      strategy: "git"
      config:
        repo_url: "https://github.com/python/peps"
        shallow: true

  - id: "evalplus_mbppplus"
    name: "MBPP+ (EvalPlus) - curated programming problems"
    enabled: true
    bucket_hint: "GREEN"
    license_profile: "permissive"
    license_evidence:
      spdx_hint: "Apache-2.0"
      url: "https://huggingface.co/datasets/evalplus/mbppplus"
    data_type: ["code","unit_tests","instructions"]
    download:
      strategy: "huggingface_datasets"
      config:
        dataset_id: "evalplus/mbppplus"
    canonicalize:
      text_field_candidates: ["prompt", "solution", "code", "text"]

  # -----------------------------
  # YELLOW (quarantine then scrub/allowlist)
  # -----------------------------

  - id: "ibm_project_codenet"
    name: "IBM Project CodeNet (large code solutions + IO tests)"
    enabled: false
    bucket_hint: "YELLOW"
    license_profile: "permissive"
    license_evidence:
      spdx_hint: "CDLA-Permissive-2.0"
      url: "https://developer.ibm.com/technologies/artificial-intelligence/data/project-codenet/"
      notes: "May require click-through/registration; enable once you have a programmatic download path."
    data_type: ["code","unit_tests","problem_statements"]
    download:
      strategy: "http"
      config:
        url: "https://developer.ibm.com/technologies/artificial-intelligence/data/project-codenet/"

  - id: "common_pile_github_archive"
    name: "Common Pile: GitHub Archive (license-filtered discussions)"
    enabled: false
    bucket_hint: "YELLOW"
    license_profile: "record_level"
    license_evidence:
      spdx_hint: "MIXED"
      url: "https://huggingface.co/datasets/common-pile/github_archive"
      notes: "Common Pile applies license filtering; still treat as record-level and keep provenance."
    data_type: ["qa","discussion","documentation"]
    download:
      strategy: "huggingface_datasets"
      config:
        dataset_id: "common-pile/github_archive"
    canonicalize:
      text_field_candidates: ["content", "text", "code", "body", "readme", "message", "comment"]

  - id: "common_pile_stackv2_edu_filtered"
    name: "Common Pile: Stack v2 (edu filtered)"
    enabled: false
    bucket_hint: "YELLOW"
    license_profile: "record_level"
    license_evidence:
      spdx_hint: "MIXED"
      url: "https://huggingface.co/datasets/common-pile/stackv2_edu_filtered"
    data_type: ["code"]
    download:
      strategy: "huggingface_datasets"
      config:
        dataset_id: "common-pile/stackv2_edu_filtered"
    canonicalize:
      text_field_candidates: ["text", "content", "body", "question", "answer", "title"]

  - id: "bigcode_the_stack"
    name: "BigCode The Stack (permissively-licensed code, with provenance)"
    enabled: false
    bucket_hint: "YELLOW"
    license_profile: "record_level"
    license_evidence:
      spdx_hint: "MIXED"
      url: "https://huggingface.co/datasets/bigcode/the-stack"
      notes: "Must follow original licenses & keep attribution/provenance."
    data_type: ["code"]
    download:
      strategy: "huggingface_datasets"
      config:
        dataset_id: "bigcode/the-stack"
    canonicalize:
      text_field_candidates: ["content", "text", "code"]

  - id: "bigcode_the_stack_v2"
    name: "BigCode The Stack v2 (very large; additional ToU/process constraints)"
    enabled: false
    bucket_hint: "YELLOW"
    license_profile: "record_level"
    license_evidence:
      spdx_hint: "MIXED"
      url: "https://huggingface.co/datasets/bigcode/the-stack-v2/tree/main"
      notes: "Bulk download requires agreement; treat as YELLOW until compliance + infra are in place."
    data_type: ["code"]
    download:
      strategy: "huggingface_datasets"
      config:
        dataset_id: "bigcode/the-stack-v2"
    canonicalize:
      text_field_candidates: ["content", "text", "code"]

  - id: "code_search_net"
    name: "CodeSearchNet (comment-code pairs; mixed licenses)"
    enabled: false
    bucket_hint: "YELLOW"
    license_profile: "record_level"
    license_evidence:
      spdx_hint: "MIXED"
      url: "https://github.com/github/CodeSearchNet"
      notes: "License data exists but is not always attached per example; add license join before GREEN."
    data_type: ["code","documentation"]
    download:
      strategy: "huggingface_datasets"
      config:
        dataset_id: "code_search_net"
        config: "python"
    canonicalize:
      text_field_candidates: ["func_code_string", "whole_func_string", "code", "text", "content", "docstring"]

  - id: "microsoft_codexglue"
    name: "CodeXGLUE (benchmark suite; C-UDA data license)"
    enabled: false
    bucket_hint: "YELLOW"
    license_profile: "unknown"
    license_evidence:
      spdx_hint: "C-UDA-1.0"
      url: "https://github.com/microsoft/CodeXGLUE"
      notes: "C-UDA is a computational-use agreement; treat as YELLOW pending legal review for training."
    data_type: ["code","documentation","qa"]
    download:
      strategy: "git"
      config:
        repo_url: "https://github.com/microsoft/CodeXGLUE"
        shallow: true

  - id: "xlangai_ds1000"
    name: "DS-1000 (data science code generation; derived from StackOverflow)"
    enabled: false
    bucket_hint: "YELLOW"
    license_profile: "copyleft"
    license_evidence:
      spdx_hint: "CC-BY-SA-4.0"
      url: "https://huggingface.co/datasets/xlangai/DS-1000"
      notes: "CC-BY-SA is share-alike; only ingest if you can satisfy attribution + SA obligations."
    data_type: ["code","instructions","unit_tests"]
    download:
      strategy: "huggingface_datasets"
      config:
        dataset_id: "xlangai/DS-1000"
    canonicalize:
      text_field_candidates: ["prompt", "solution", "reference_solution", "code", "text"]

  # (Intentionally no RED targets in this file; keep RED candidates in the markdown plan)
