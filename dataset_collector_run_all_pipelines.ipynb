{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Collector \u2014 Run All Pipelines\n",
        "\n",
        "This notebook orchestrates every `*_pipeline_v2` directory in this repository.\n",
        "\n",
        "* It expects to run inside the conda environment named **`Dataset_Collector`**.\n",
        "* It prompts for API keys (for example `GITHUB_TOKEN`, `CHEMSPIDER_API_KEY`) when missing,\n",
        "  and sets them as environment variables for the current session.\n",
        "* It can optionally install each pipeline's requirements before running stages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "NOTEBOOK_NAME = 'dataset_collector_run_all_pipelines.ipynb'\n",
        "\n",
        "def locate_repo_root(start: Path) -> Path:\n",
        "    if (start / NOTEBOOK_NAME).exists():\n",
        "        return start\n",
        "    for parent in start.parents:\n",
        "        if (parent / NOTEBOOK_NAME).exists():\n",
        "            return parent\n",
        "    for child in start.iterdir():\n",
        "        if child.is_dir() and (child / NOTEBOOK_NAME).exists():\n",
        "            return child\n",
        "    return start\n",
        "\n",
        "repo_root = locate_repo_root(Path.cwd())\n",
        "if repo_root != Path.cwd():\n",
        "    os.chdir(repo_root)\n",
        "    print(f'Changed working directory to repo root: {repo_root}')\n",
        "elif not (repo_root / NOTEBOOK_NAME).exists():\n",
        "    print(\n",
        "        'WARNING: Could not locate notebook in current or nearby directories. '\n",
        "        'If pipelines are not detected, set repo_root manually.'\n",
        "    )\n",
        "\n",
        "print(f'Working directory: {repo_root}')\n",
        "conda_env = os.environ.get('CONDA_DEFAULT_ENV')\n",
        "if conda_env != 'Dataset_Collector':\n",
        "    print(\n",
        "        'WARNING: Expected conda env Dataset_Collector, '\n",
        "        f'but CONDA_DEFAULT_ENV={conda_env!r}.\\n',\n",
        "        'Activate the correct env before proceeding.'\n",
        "    )\n",
        "else:\n",
        "    print('Conda env looks correct: Dataset_Collector')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_dirs = sorted([p for p in repo_root.iterdir() if p.is_dir() and p.name.endswith('_pipeline_v2')])\n",
        "print('Detected pipelines:')\n",
        "for pipeline in pipeline_dirs:\n",
        "    print(f'  - {pipeline.name}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "FIX_PERMISSIONS = False  # Set True to chmod +x missing run_pipeline.sh permissions.\n",
        "\n",
        "missing_scripts = []\n",
        "non_executable = []\n",
        "for pipeline in pipeline_dirs:\n",
        "    run_script = pipeline / 'run_pipeline.sh'\n",
        "    if not run_script.exists():\n",
        "        missing_scripts.append(pipeline.name)\n",
        "        continue\n",
        "    if not os.access(run_script, os.X_OK):\n",
        "        non_executable.append(run_script)\n",
        "\n",
        "if missing_scripts:\n",
        "    print('Pipelines missing run_pipeline.sh:', ', '.join(missing_scripts))\n",
        "else:\n",
        "    print('All pipelines have run_pipeline.sh.')\n",
        "\n",
        "if non_executable:\n",
        "    print('Non-executable run_pipeline.sh found:')\n",
        "    for script in non_executable:\n",
        "        print(f'  - {script}')\n",
        "    if FIX_PERMISSIONS:\n",
        "        for script in non_executable:\n",
        "            script.chmod(script.stat().st_mode | 0o111)\n",
        "        print('Updated permissions for run_pipeline.sh scripts.')\n",
        "    else:\n",
        "        print('Set FIX_PERMISSIONS=True to chmod +x.')\n",
        "else:\n",
        "    print('All run_pipeline.sh scripts are executable.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "\n",
        "def ensure_env(var_name: str, prompt: str) -> None:\n",
        "    current = os.environ.get(var_name, '').strip()\n",
        "    if current:\n",
        "        print(f'{var_name} already set (length={len(current)})')\n",
        "        return\n",
        "    value = getpass.getpass(f'{prompt} (leave blank to skip): ')\n",
        "    if value:\n",
        "        os.environ[var_name] = value\n",
        "        print(f'Set {var_name} for this session.')\n",
        "    else:\n",
        "        print(f'Skipped {var_name}. Some pipeline sources may fail or rate-limit.')\n",
        "\n",
        "required_env_prompts = {\n",
        "    'GITHUB_TOKEN': 'Enter a GitHub token for higher rate limits',\n",
        "    'CHEMSPIDER_API_KEY': 'Enter a ChemSpider API key (chemistry pipeline)',\n",
        "}\n",
        "\n",
        "for var_name, prompt in required_env_prompts.items():\n",
        "    ensure_env(var_name, prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "INSTALL_REQUIREMENTS = False  # Set to True to install all pipeline requirements.\n",
        "\n",
        "if INSTALL_REQUIREMENTS:\n",
        "    for pipeline in pipeline_dirs:\n",
        "        requirements = pipeline / 'requirements.txt'\n",
        "        if requirements.exists():\n",
        "            print(f'Installing requirements for {pipeline.name}...')\n",
        "            subprocess.run(\n",
        "                [sys.executable, '-m', 'pip', 'install', '-r', str(requirements)],\n",
        "                check=True,\n",
        "            )\n",
        "        else:\n",
        "            print(f'No requirements.txt found for {pipeline.name}, skipping.')\n",
        "else:\n",
        "    print('Skipping requirements install. Set INSTALL_REQUIREMENTS=True to enable.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "STAGES = [\n",
        "    'classify',\n",
        "    'acquire_green',\n",
        "    'acquire_yellow',\n",
        "    'screen_yellow',\n",
        "    'merge',\n",
        "    'difficulty',\n",
        "    'catalog',\n",
        "]\n",
        "EXECUTE = False  # Set True to perform writes; False = dry-run.\n",
        "\n",
        "mode_flag = '--execute' if EXECUTE else '--dry-run'\n",
        "env = os.environ.copy()\n",
        "\n",
        "for pipeline in pipeline_dirs:\n",
        "    run_script = pipeline / 'run_pipeline.sh'\n",
        "    if not run_script.exists():\n",
        "        print(f'Skipping {pipeline.name}: run_pipeline.sh not found.')\n",
        "        continue\n",
        "    print(f'\\n=== Running {pipeline.name} ({mode_flag}) ===')\n",
        "    for stage in STAGES:\n",
        "        cmd = ['bash', str(run_script), '--stage', stage, mode_flag]\n",
        "        print(' '.join(cmd))\n",
        "        subprocess.run(cmd, check=True, env=env, cwd=pipeline)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}