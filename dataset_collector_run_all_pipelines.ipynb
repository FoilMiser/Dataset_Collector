{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Dataset Collector \u2014 Run All Pipelines\n\nThis notebook orchestrates every `*_pipeline_v2` directory in this repository.\n\n* It expects to run inside the conda environment named **`Dataset_Collector`**.\n* It prompts for API keys (for example `GITHUB_TOKEN`, `CHEMSPIDER_API_KEY`) when missing,\n  and sets them as environment variables for the current session.\n* It can optionally install each pipeline's requirements before running stages.\n\n\n**Set `DEST_ROOT`** to your dataset output directory before running any pipeline stages."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python tools/validate_repo.py --root .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "NOTEBOOK_NAME = 'dataset_collector_run_all_pipelines.ipynb'\n",
        "\n",
        "def locate_repo_root(start: Path) -> Path:\n",
        "    if (start / NOTEBOOK_NAME).exists():\n",
        "        return start\n",
        "    for parent in start.parents:\n",
        "        if (parent / NOTEBOOK_NAME).exists():\n",
        "            return parent\n",
        "    for child in start.iterdir():\n",
        "        if child.is_dir() and (child / NOTEBOOK_NAME).exists():\n",
        "            return child\n",
        "    return start\n",
        "\n",
        "repo_root = locate_repo_root(Path.cwd())\n",
        "if repo_root != Path.cwd():\n",
        "    os.chdir(repo_root)\n",
        "    print(f'Changed working directory to repo root: {repo_root}')\n",
        "elif not (repo_root / NOTEBOOK_NAME).exists():\n",
        "    print(\n",
        "        'WARNING: Could not locate notebook in current or nearby directories. '\n",
        "        'If pipelines are not detected, set repo_root manually.'\n",
        "    )\n",
        "\n",
        "print(f'Working directory: {repo_root}')\n",
        "conda_env = os.environ.get('CONDA_DEFAULT_ENV')\n",
        "if conda_env != 'Dataset_Collector':\n",
        "    print(\n",
        "        'WARNING: Expected conda env Dataset_Collector, '\n",
        "        f'but CONDA_DEFAULT_ENV={conda_env!r}.\\n',\n",
        "        'Activate the correct env before proceeding.'\n",
        "    )\n",
        "else:\n",
        "    print('Conda env looks correct: Dataset_Collector')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_dirs = sorted([p for p in repo_root.iterdir() if p.is_dir() and p.name.endswith('_pipeline_v2')])\n",
        "print('Detected pipelines:')\n",
        "for pipeline in pipeline_dirs:\n",
        "    print(f'  - {pipeline.name}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "\n",
        "def ensure_env(var_name: str, prompt: str) -> None:\n",
        "    current = os.environ.get(var_name, '').strip()\n",
        "    if current:\n",
        "        print(f'{var_name} already set (length={len(current)})')\n",
        "        return\n",
        "    value = getpass.getpass(f'{prompt} (leave blank to skip): ')\n",
        "    if value:\n",
        "        os.environ[var_name] = value\n",
        "        print(f'Set {var_name} for this session.')\n",
        "    else:\n",
        "        print(f'Skipped {var_name}. Some pipeline sources may fail or rate-limit.')\n",
        "\n",
        "required_env_prompts = {\n",
        "    'GITHUB_TOKEN': 'Enter a GitHub token for higher rate limits',\n",
        "    'CHEMSPIDER_API_KEY': 'Enter a ChemSpider API key (chemistry pipeline)',\n",
        "}\n",
        "\n",
        "for var_name, prompt in required_env_prompts.items():\n",
        "    ensure_env(var_name, prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Set to True to install all pipeline requirements (recommended on first run).\n",
        "INSTALL_REQUIREMENTS = False\n",
        "\n",
        "if INSTALL_REQUIREMENTS:\n",
        "    for pipeline in pipeline_dirs:\n",
        "        requirements = pipeline / 'requirements.txt'\n",
        "        if requirements.exists():\n",
        "            print(f'Installing requirements for {pipeline.name}...')\n",
        "            subprocess.run(\n",
        "                [sys.executable, '-m', 'pip', 'install', '-r', str(requirements)],\n",
        "                check=True,\n",
        "            )\n",
        "        else:\n",
        "            print(f'No requirements.txt found for {pipeline.name}, skipping.')\n",
        "else:\n",
        "    print('Skipping requirements install. Set INSTALL_REQUIREMENTS=True to enable.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import subprocess\nimport sys\nfrom pathlib import Path\n\n# Windows-first runner (recommended)\n# Uses tools/build_natural_corpus.py to orchestrate pipelines without bash.\nDEST_ROOT = r\"E:\\AI-Research\\datasets\\Natural\"  # Update to your dataset root.\nEXECUTE = False  # True = downloads/writes; False = plan-only.\nWORKERS = 6  # acquisition parallelism\nMODE = \"collect\"  # collect | compile | full\n\nrepo_root = Path(repo_root)\nvalidator = [sys.executable, str(repo_root / \"tools\" / \"validate_repo.py\")]\nprint(\" \".join(validator))\nsubprocess.run(validator, check=True, cwd=str(repo_root))\n\ncmd = [\n    sys.executable, str(repo_root / \"tools\" / \"build_natural_corpus.py\"),\n    \"--dest-root\", DEST_ROOT,\n    \"--workers\", str(WORKERS),\n    \"--mode\", MODE,\n]\nif EXECUTE:\n    cmd.append(\"--execute\")\n\nprint(\" \".join(cmd))\nsubprocess.run(cmd, check=True, cwd=str(repo_root))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Manual YELLOW review (required before screening)\n",
        "\n",
        "1. Run the **collect** stages (classify/acquire).\n",
        "2. Review the YELLOW queue and approve or reject targets:\n",
        "   - `python <pipeline>/review_queue.py list --queue <dataset_root>/_queues/yellow_pipeline.jsonl`\n",
        "   - `python <pipeline>/review_queue.py approve --target <target_id> --manifest-dir <dataset_root>/_manifests/<target_id>`\n",
        "   - `python <pipeline>/review_queue.py reject --target <target_id> --manifest-dir <dataset_root>/_manifests/<target_id>`\n",
        "3. Re-run the **compile** stages (screen/merge/catalog).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Appendix: WSL/Git Bash runner (optional)\n\nUse this only if you prefer bash-based per-pipeline execution."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "import yaml\n\n",
        "# Advanced: bash-based per-pipeline runner (WSL/Git Bash required)\n",
        "STAGES = [\n",
        "    \"classify\",\n",
        "    \"acquire_green\",\n",
        "    \"acquire_yellow\",\n",
        "    \"screen_yellow\",\n",
        "    \"merge\",\n",
        "    \"catalog\",\n",
        "]\n",
        "EXECUTE = False  # Set True to perform writes; False = dry-run.\n\n",
        "repo_root = Path(repo_root)\n\n",
        "pipeline_map_path = repo_root / \"tools\" / \"pipeline_map.yaml\"\n",
        "pipeline_map = yaml.safe_load(pipeline_map_path.read_text(encoding=\"utf-8\"))\n",
        "targets_for = {k: v[\"targets_yaml\"] for k, v in (pipeline_map.get(\"pipelines\") or {}).items()}\n\n",
        "env = os.environ.copy()\n\n",
        "for pipeline in pipeline_dirs:\n",
        "    run_script = pipeline / \"run_pipeline.sh\"\n",
        "    if not run_script.exists():\n",
        "        print(f\"Skipping {pipeline.name}: run_pipeline.sh not found.\")\n",
        "        continue\n\n",
        "    targets_name = targets_for.get(pipeline.name)\n",
        "    if not targets_name:\n",
        "        raise RuntimeError(f\"No targets_yaml entry for {pipeline.name} in tools/pipeline_map.yaml\")\n",
        "    targets_path = pipeline / targets_name\n",
        "    if not targets_path.exists():\n",
        "        raise RuntimeError(f\"Targets YAML not found for {pipeline.name}: {targets_path}\")\n\n",
        "    print(f\"\\n=== Running {pipeline.name} ({'EXECUTE' if EXECUTE else 'DRY'}) ===\")\n",
        "    for stage in STAGES:\n",
        "        cmd = [\"bash\", str(run_script), \"--targets\", str(targets_path), \"--stage\", stage]\n",
        "        if EXECUTE:\n",
        "            cmd.append(\"--execute\")\n",
        "        print(\" \".join(cmd))\n",
        "        subprocess.run(cmd, check=True, env=env, cwd=pipeline)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}