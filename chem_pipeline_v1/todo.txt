================================================================================
TODO — Chemistry Corpus Pipeline
================================================================================

Legend:
  [x] = Completed
  [~] = Partially addressed / MVP landed
  [ ] = Not yet implemented

================================================================================
VERSION HISTORY
================================================================================

v0.5 -> v0.6 Completed:
  [x] Retry with exponential backoff
  [x] Resumable HTTP downloads (range requests)
  [x] Integrity verification (SHA256 + Zenodo MD5)
  [x] Parallel downloads with workers
  [x] Versioned field schemas (field_schemas.yaml)
  [x] Schema validation + type casting
  [x] PubChem computed-only extraction (CID-range sharding + resume)
  [x] PMC OA allowlist parsing with fallbacks
  [x] PMC tarball caching
  [x] Enhanced JATS parsing (sections, captions)
  [x] Global catalog builder
  [x] Training manifest composer
  [x] Wrapper script (run_pipeline.sh)
  [x] Dry-run summary reports

v0.6 -> v0.7 Shipped:
  [x] Explicit denylist support (denylist.yaml + pipeline_driver integration)
  [x] Manual review helper (review_queue.py) writing review_signoff.json
  [x] Review gating in pipeline_driver:
      - reject forces RED
      - review_required forces YELLOW until approved
      - approve can optionally promote_to GREEN (explicit)
  [x] Wrapper script stage: review (lists pending YELLOW items)

v0.7 -> v0.8 Shipped:
  [x] Review Workflow: enforce + audit
      - globals.require_yellow_signoff support
      - Extended signoff schema (evidence_links_checked, constraints, notes, reviewer_contact)
      - `review_queue.py export` command (CSV/JSON)
  [x] Denylist: expand semantics
      - Structured patterns (domain extraction, publisher/provider tags)
      - Per-pattern severity (hard_red, force_yellow)
      - Denylist provenance (link/rationale fields mandatory)
  [x] License evidence snapshots: make "offline" safer
      - When `--no-fetch` is set: require existing license_evidence snapshot
      - Otherwise force YELLOW (no implicit GREEN based on hints)
  [x] Dataset-aware splitting (leak prevention)
      - Implemented `split_group_id` for related artifacts
      - Split reports (counts + token estimates)
  [x] Near-duplicate detection (beyond exact hash)
      - Optional MinHash/LSH stage configuration
      - Emit duplicate groups into catalog
  [x] Record-level filtering: broaden coverage
      - Added MoNA (MassBank of North America)
      - Added GNPS (Global Natural Products Social Networking)
      - Added ChemSpider (requires API key)
  [x] Parquet output option
      - `--emit-parquet` configuration
      - Compression options (snappy, gzip, zstd)
  [x] Resolver expansion / robustness
      - Enhanced Figshare resolver with API v2
      - GitHub release resolver with rate limiting
  [x] InChIKey / SMILES normalization (optional)
      - Optional RDKit-based normalizer configuration
      - Normalization coverage in catalog stats

v0.8 -> v0.9 Shipped:
  [x] Parallel + incremental processing
      - Async/concurrent license evidence fetching with HTTP pooling
      - Parallel catalog building with progress bars for large pools
      - Delta-only runs, incremental catalog updates, and checkpointed resumes
  [x] Audit trail + provenance
      - Cryptographic signatures for evaluation.json files
      - Append-only audit log with pipeline run IDs
      - Full provenance chain captured per decision
  [x] Resilience + recovery
      - Checkpoint/resume for downloads and catalog stages
      - Circuit breaker retries and partial-download recovery
      - Health checks for critical external services
  [x] Deduplication + spectra coverage
      - MinHash/LSH deduplication with cross-dataset reports
      - Fuzzy chemical synonym matching and spectrum-based deduplication hooks
      - MSP/MGF parsing and normalization for MoNA/GNPS

v0.9 -> v1.0 Shipped (Production Readiness):
  [x] License verification + compliance hardening
      - Confidence scoring for SPDX resolution + automated pattern checks
      - License change detection by comparing evidence snapshots
      - Enhanced web scraping for dynamic/license-gated pages
      - Documented legal review checklist for RED/YELLOW edge cases
  [x] Governance + review UX
      - Batch approval/rejection with reviewer assignment/notifications
      - Run-level audit views (surface run IDs, signatures, provenance)
      - Access controls for reviewers and approvers
      - Visual diff support for license evidence changes
  [x] Testing + CI/CD
      - Unit tests for license normalization and resolvers
      - Integration tests for download workers and scrubbers with sample data
      - End-to-end smoke tests (green/yellow/red queues + catalog build)
      - CI pipeline (lint/type checks/tests) with artifact retention
  [x] Deployment + packaging
      - Docker support for reproducible runs
      - Kubernetes/Helm job templates with secrets/volume wiring
      - Versioned configuration bundles (schemas + denylist + defaults)
      - Publish pinned requirements/lockfiles
  [x] RDKit + structure processing
      - Hardened SMILES canonicalization and InChIKey validation
      - Molecular property calculation + substructure filtering hooks
      - Performance tuning for large SDF ingestion

================================================================================
v1.0 -> v1.1 — WHAT NEEDS TO CHANGE / BE UPDATED
================================================================================

HIGH PRIORITY (Scaling + Robustness)
------------------------------------

1) Data coverage expansions
   [ ] Add ZINC database pipeline with licensing verification
   [ ] Add BindingDB pipeline with affinity data extraction
   [ ] Add DrugBank pipeline (where licensed)
   [ ] Add USPTO patent chemistry extraction
   [ ] Expand spectrum ingestion for additional MSP/MGF sources

2) Monitoring + alerting
   [ ] Metrics on download times, error rates, queue throughput
   [ ] Prometheus/StatsD exporters with example dashboards
   [ ] Grafana dashboard templates for pipeline health
   [ ] Slack/email notifications for failures or stalled queues
   [ ] SLA monitoring for critical data sources

3) Web dashboard for governance
   [ ] Flask/FastAPI dashboard for YELLOW review with visual diffs
   [ ] Real-time queue status and progress visualization
   [ ] Interactive approval/rejection with audit trail
   [ ] Role-based access control (RBAC) for reviewers

4) Documentation + onboarding
   [ ] Architecture doc with detailed diagrams
   [ ] Tutorial for adding new targets and resolvers
   [ ] Troubleshooting guide and FAQ for common failures
   [ ] API/programmatic access examples
   [ ] Video walkthroughs for common workflows

MEDIUM PRIORITY (Quality + Ergonomics)
--------------------------------------

5) Performance + streaming
   [ ] Streaming processing for large files (>10GB) with memory profiling
   [ ] Optional progress dashboards (CLI + web) for long jobs
   [ ] Configurable caching/prefetch for common resolvers
   [ ] Batch processing optimizations for catalog builds

6) Cross-dataset intelligence
   [ ] Cross-dataset deduplication reports with structure + spectra IDs
   [ ] Compound identity resolution across sources
   [ ] Conflict detection for contradictory data
   [ ] Data quality scoring per source

7) Container/orchestration polish
   [ ] Helm chart hardening (resource quotas, pod disruption budgets)
   [ ] Airflow/Argo workflow examples for scheduled runs
   [ ] Backup/restore procedures for catalogs and manifests
   [ ] Multi-region deployment support

8) Advanced license analysis
   [ ] Machine learning-based license classification assistance
   [ ] Integration with legal review tools (e.g., FOSSA, Snyk)
   [ ] License compatibility matrix generation
   [ ] Automated license change alerts

LOW PRIORITY (Future-proofing)
------------------------------

9) Multi-language + international support
   [ ] Multi-language support for restriction phrase scanning
   [ ] International license variants detection
   [ ] Localized documentation

10) Advanced integrations
    [ ] Integration with data versioning tools (DVC, LakeFS)
    [ ] Support for streaming/real-time data sources
    [ ] Federated catalog support (multiple organizations)
    [ ] API for programmatic access to catalog

================================================================================
BACKLOG (Future versions)
================================================================================

- Support for proprietary dataset agreements (with encryption)
- Differential privacy integration for sensitive data
- Automated report generation for compliance audits
- Real-time license monitoring for web sources
- GraphQL API for catalog queries
- Plugin architecture for custom resolvers
- Distributed processing with Ray/Dask
- Model-specific data preparation pipelines
- A/B testing infrastructure for data quality experiments

================================================================================
NOTES
================================================================================
- Keep RED items out of training manifests, regardless of local copies.
- Prefer computed-only transforms for ambiguous sources.
- Always snapshot license/terms evidence before large downloads.
- Use split_group_id to prevent data leakage across train/valid splits.
- Test thoroughly before deploying to production.
- Monitor external data sources for license changes.
